{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.matplotlib.rcParams['savefig.dpi'] = 144\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Engine, Session 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem definition and data format\n",
    "\n",
    "The goal of a recommendation engine is to match items to users that will prefer them. We can imagine this being extremely useful across a wide variety of e-commerce applications, for starters.\n",
    "\n",
    "For this recommendation engine, we start with a set of user ratings. We can use these historical ratings to make guesses about which future items a user will rate highest.  In this tutorial, we will cover two ways of doing this:\n",
    "\n",
    "1. **Content-based rating:** We can make this guess based only on features of the users or items.\n",
    "1. **Collaborative Rating:** We could also look at ratings of similar users or similar items.\n",
    "\n",
    "For an example data set, we are using the [MovieLens 10M dataset](http://files.grouplens.org/datasets/movielens/ml-10m.zip).  This contains about 10M ratings for 10k movies by 72k users.  We will be building applications that attempts to present movies that a given user would rate highly.  Extracting the zip file in the current directory should create a directory called `ml-10M100K/`.  The data wil be in several `.dat` files.  Let's take a look at the list of movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ml-10M100K/movies.dat', 'r') as f:\n",
    "    for i in xrange(5):\n",
    "        print f.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line has three sections, separated by `::`.  It's easy enough to split them and read each line into a dictionary.\n",
    "1. The first element is a key, which we will need to identify the movie in the other files.\n",
    "1. Next is the title, with the year included for disambiguation.\n",
    "1. We'll go ahead and read that out as a separate feature.\n",
    "1. The last group is a list of categories, separated by `|`.  We'll convert this to a list.\n",
    "\n",
    "The `parse_movie_line` function does that for a single line, returning a dictionary of key-value pairs.  Using list comprehension, we can easily run over the entire file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_movie_line(l):\n",
    "    id_, title, cats = l.strip().split('::')\n",
    "    return {'id': int(id_), 'title': title, 'year': int(title.rsplit(' ')[-1][1:-1]), \n",
    "            'categories': cats.split('|')}\n",
    "\n",
    "with open('ml-10M100K/movies.dat', 'r') as f:\n",
    "    movies = [parse_movie_line(l) for l in f]\n",
    "\n",
    "movies[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is using the built-in Python data types of lists and dictionaries. We'll get much better performance with the *Pandas* module.\n",
    "\n",
    "If we feed Pandas our list of dictionaries, it does the right thing, making each dictionary a row, with the columns given by the dictionary keys.  By default, Pandas will assign a numerical index to the rows.  Since we have an id key for the movies, we'll use that as the index instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(movies).set_index('id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do the same thing with the tags and ratings data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tag_line(l):\n",
    "    uid, mid, tag, time = l.strip().split('::')\n",
    "    return {'user_id': int(uid), 'movie_id': int(mid), 'tag': tag.lower()}\n",
    "\n",
    "with open('ml-10M100K/tags.dat', 'r') as f:\n",
    "    df_tags = pd.DataFrame([parse_tag_line(l) for l in f])\n",
    "\n",
    "df_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def parse_rating_line(l):\n",
    "    uid, mid, rating, time = l.strip().split('::')\n",
    "    return {'user_id': int(uid), 'movie_id': int(mid), 'rating': float(rating)}\n",
    "\n",
    "with open('ml-10M100K/ratings.dat', 'r') as f:\n",
    "    df_ratings = pd.DataFrame([parse_rating_line(l) for l in f])\n",
    "\n",
    "df_ratings.head()\n",
    "'''\n",
    "import itertools\n",
    "def parse_rating_line(l):\n",
    "    uid, mid, rating, time = l.strip().split('::')\n",
    "    return {'user_id': int(uid), 'movie_id': int(mid), 'rating': float(rating)}\n",
    "\n",
    "with open('ml-10M100K/ratings.dat', 'r') as f:\n",
    "    df_ratings = pd.DataFrame((parse_rating_line(l)\n",
    "                               for l in itertools.islice(f, 2000000)))\n",
    "\n",
    "df_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring a New Dataset\n",
    "\n",
    "One of the most important aspects of practical data science work is being able to effectively understand your data. Luckily, Pandas + Jupyter make this easy for us. Let's first take a look at our movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_scores = df_ratings.groupby(\"movie_id\")[\"rating\"].agg([\"mean\", \"count\", \"sum\"]).join(df).sort_values(\"mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat((avg_scores[[\"mean\", \"count\", \"title\"]].head(), # head() gives us the highest rated movies\n",
    "           avg_scores[[\"mean\", \"count\", \"title\"]].tail())) # and tail() gives us the lowest rated ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediately notice our first issue: some of our highest and lowest-rated movies have few ( < 100) reviews. A question that immediately comes to mind is whether or not people rate popular movies differently than unpopular ones. A quick plot indicates this probably isn't the case.\n",
    "\n",
    "If we wanted to do a more focused analysis, we could compare these distributions with statistical tests, but the purpose of this notebook is to develop a recommendation engine, so we stay focused on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = avg_scores[\"mean\"].hist(label=\"all scores\")\n",
    "avg_scores[avg_scores[\"count\"] > 100][\"mean\"].hist(ax=ax, label=\"n > 100\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, it's enough to pick the simplest possible solutions and iterate from there. As a first stab, let's throw together a couple of extremely simple recommenders:\n",
    "\n",
    "1. A \"Most Popular\" recommender - we'll use number of reviews as our metric for \"best\".\n",
    "2. A \"Highest Rated\" recommender - this one will be a little more tricky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_popular_recommender(user_id, num=10):\n",
    "    most_popular = avg_scores.sort_values(\"count\", ascending=False)[:num]\n",
    "    return most_popular\n",
    "\n",
    "most_popular_recommender(2)[[\"title\", \"mean\", \"count\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highest Rated Recommender\n",
    "\n",
    "As we saw in our exploration above, we have to worry about the fact that some of our highest-scoring movies have only been rated by one or two users. They're probably not movies we want to recommend as \"top rated\" to end-users.\n",
    "\n",
    "\n",
    "### Average Rating Try 1: Naive Sum\n",
    "\n",
    "Using the sum of the ratings will solve this particular problem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings.groupby(\"movie_id\")[\"rating\"].agg([\"sum\"]).join(df).sort_values('sum', ascending=False)[[\"title\", \"sum\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... but will tend to highlight only those popular movies that almost everyone has seen. We could use this as a metric for \"most popular\" movies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try 2: Bayesian Smoothing\n",
    "\n",
    "One way to balance these two competing needs is to introduce a **Bayesian prior**.  This is the estimate we make that every movie has a rating $\\mu$, before seeing any of the user ratings.  As we see ratings from users come it, we formulate a **posterior** estimate of the rating, taking into account both the prior and the new information provided by the ratings.  The posterior estimate of the rating, after seeing the $n$ reviews $\\{x_i\\}$, can be written as\n",
    "\n",
    "$$ \\frac{ \\sum_{i=0}^n x_i + \\mu N}{n + N} \\ , $$\n",
    "\n",
    "where $N$ reflects our level of confidence in the prior.  Adjusting $N$ affects the number of user reviews needed to push the posterior estimate away from $mu$.  Adjusting $mu$ affects where in the rankings movies with few reviews appear.  This technique is known as [Bayesian Smoothing](https://en.wikipedia.org/wiki/Additive_smoothing).\n",
    "\n",
    "In this case, we use $\\mu = 3$, $N = 5$, essentially starting each movie off with five reviews of three stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bayes_sum(N, mu):\n",
    "    f = lambda x: (x.sum() + mu*N) / (x.count() + N)\n",
    "    f.__name__ = \"bayes_sum\" # a hack to make the column name appear nicely\n",
    "    return f\n",
    "\n",
    "df_ratings.groupby(\"movie_id\")[\"rating\"].agg([bayes_sum(5, 3), \"count\"])\\\n",
    "          .join(df).sort_values(\"bayes_sum\", ascending=False)[[\"title\", \"bayes_sum\", \"count\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better - this looks like a list of what we'd expect to be \"highly rated\" movies.\n",
    "\n",
    "This dataset, however, contains much deeper information about movie category. Let's start to leverage that to develop more sophisticated models and better recommendations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "1. Develop a \"by-category\" recommendation model - given a category, return the top rated movies from that category. Keep in mind the above discussion about what \"top rated\" is. This will also help you look into the dataset more closely.\n",
    "2. Play around with the Bayesian smoothing parameters. Our mean review score is closer to 3.5 , so perhaps we should try that instead of 3? Will that help pull out movies that might be good, but aren't necessarily as popular? Also think about this in your by-category model from 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Now, let's start thinking about our users and providing tailored recommendations based on their review history.\n",
    "\n",
    "If a user rated a bunch of Adventure films highly, a simple scheme would be to recommend more Adventure films.  Of course, many users may like several genres of movies, so we need a way to represent categories beyond just the string representing the favorite.\n",
    "\n",
    "As before, we're going to **vectorize** our categories into a high-dimensional geometric space. From there, we'll try to find the most similar movies given their category. We'll use a technique called K-nearest-neighbors to do this. We'll illustrate with a 2-dimensional example. Let's consider two categories, \"Romance\" and \"Comedy.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Comedy, Romance]</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Comedy, Drama, Romance]</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[Comedy, Romance]</td>\n",
       "      <td>Sabrina (1995)</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[Comedy, Drama, Romance]</td>\n",
       "      <td>American President, The (1995)</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[Comedy, Horror]</td>\n",
       "      <td>Dracula: Dead and Loving It (1995)</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[Action, Adventure, Romance]</td>\n",
       "      <td>Cutthroat Island (1995)</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[Comedy, Drama, Romance]</td>\n",
       "      <td>Sense and Sensibility (1995)</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[Comedy, Drama, Thriller]</td>\n",
       "      <td>Four Rooms (1995)</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>Ace Ventura: When Nature Calls (1995)</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[Action, Comedy, Crime, Drama, Thriller]</td>\n",
       "      <td>Money Train (1995)</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[Action, Comedy, Drama]</td>\n",
       "      <td>Get Shorty (1995)</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[Drama, Romance]</td>\n",
       "      <td>Leaving Las Vegas (1995)</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[Drama, Romance]</td>\n",
       "      <td>Persuasion (1995)</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           categories  \\\n",
       "id                                                      \n",
       "1   [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "3                                   [Comedy, Romance]   \n",
       "4                            [Comedy, Drama, Romance]   \n",
       "5                                            [Comedy]   \n",
       "7                                   [Comedy, Romance]   \n",
       "11                           [Comedy, Drama, Romance]   \n",
       "12                                   [Comedy, Horror]   \n",
       "15                       [Action, Adventure, Romance]   \n",
       "17                           [Comedy, Drama, Romance]   \n",
       "18                          [Comedy, Drama, Thriller]   \n",
       "19                                           [Comedy]   \n",
       "20           [Action, Comedy, Crime, Drama, Thriller]   \n",
       "21                            [Action, Comedy, Drama]   \n",
       "25                                   [Drama, Romance]   \n",
       "28                                   [Drama, Romance]   \n",
       "\n",
       "                                    title  year  \n",
       "id                                               \n",
       "1                        Toy Story (1995)  1995  \n",
       "3                 Grumpier Old Men (1995)  1995  \n",
       "4                Waiting to Exhale (1995)  1995  \n",
       "5      Father of the Bride Part II (1995)  1995  \n",
       "7                          Sabrina (1995)  1995  \n",
       "11         American President, The (1995)  1995  \n",
       "12     Dracula: Dead and Loving It (1995)  1995  \n",
       "15                Cutthroat Island (1995)  1995  \n",
       "17           Sense and Sensibility (1995)  1995  \n",
       "18                      Four Rooms (1995)  1995  \n",
       "19  Ace Ventura: When Nature Calls (1995)  1995  \n",
       "20                     Money Train (1995)  1995  \n",
       "21                      Get Shorty (1995)  1995  \n",
       "25               Leaving Las Vegas (1995)  1995  \n",
       "28                      Persuasion (1995)  1995  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rom_coms = df[df.categories.apply(lambda x: 'Romance' in x or 'Comedy' in x)][:15]\n",
    "rom_coms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first approach for vectorizing the categories might be to assign numbers to each of the categories.  For example, we could say *Adventure* = 1, *Drama* = 2, *Comedy* = 3, and so on.  This approach, however, signals that there is some order to these genres.  This mapping would suggest that *Drama* = (*Adventure* + *Comedy*)/2, which isn't meaningful.  We don't want a movie that is an *Adventure Comedy* to have the same feature as a *Drama*.\n",
    "\n",
    "Instead, we need to recognize that genre is a **categorical variable**, and thus should be encoded with **one-hot encoding**.  Essentially, this give each genre its own dimension in feature space.  Our *Rom-Com* movie will be located along both the *Romantic* and *Comedy* axes, but at 0 along the *Drama* axis. Let's first demonstrate this along the two dimensions of Romance and Comedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rom_com_dicts = rom_coms.categories.apply(lambda x: {\"Romance\": (1 if 'Romance' in x else 0), \n",
    "                                          \"Comedy\": (1 if 'Comedy' in x else 0)})\n",
    "rom_com_dicts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use *Scikit Learn's* **transformers** to encode this into a list of feature vectors. In particular, we'll use a `DictVectorizer` - a tool that conveniently one-hot encodes categories provided in dictionary format.\n",
    "\n",
    "With two dimensions (Romance and Comedy), the `DictVectorizer` is perhaps overkill - however, for the general case, we don't know the categories *a priori*, as we did with the days of the week. In this case, the `DictVectorizer` will help us.  This transformer takes in a list of dictionaries.  Each key in the dictionaries gets mapped to a column, and the values for those keys are placed in the appropriate column.  Columns for keys that are not present in a particular row are filled with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "rom_com_features = DictVectorizer().fit_transform(rom_com_dicts)\n",
    "rom_com_features.toarray() # we call toarray() because sklearn gives us back sparse matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first column is our \"comedy\" feature, and the second column is \"romance\". We can plot these two dimensions (adding a bit of random noise so the points aren't directly on top of each other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rom_com_features.shape\n",
    "x_comedy = rom_com_features.toarray()[:, 0] + np.random.normal(scale=.1, size=rom_com_features.shape[0])\n",
    "y_romance = rom_com_features.toarray()[:, 1] + np.random.normal(scale=.1, size=rom_com_features.shape[0])\n",
    "\n",
    "plt.scatter(x_comedy, y_romance)\n",
    "plt.xlabel(\"Comedy\")\n",
    "plt.ylabel(\"Romance\")\n",
    "\n",
    "for label, x, y in zip(rom_coms.title, x_comedy, y_romance):\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy=(x, y), xytext=(-20, 20),\n",
    "        textcoords='offset points', ha='right', va='bottom',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "        arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code now generalizes this into many dimensions - using the `DictEncoder` class to take our list of categories and turn it into a dictionary with the same format as the one above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import base\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DictEncoder(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, col):\n",
    "        self.col = col\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        def to_dict(l):\n",
    "            try:\n",
    "                return {x: 1 for x in l}\n",
    "            except TypeError:\n",
    "                return {}\n",
    "        \n",
    "        return X[self.col].apply(to_dict) if self.col else  X.apply(to_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, a pipeline helps us chain transformers together.  Since the pipeline doesn't end in an estimator, it acts as a transformer instead.\n",
    "\n",
    "Transformers have a convenience `fit_transform()` method that simply calls `fit()` and then `transform()` immediately afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_pipe = Pipeline([('encoder', DictEncoder('categories')),\n",
    "                     ('vectorizer', DictVectorizer())])\n",
    "features = cat_pipe.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DictVectorizer` returns a sparse matrix.  This allows efficient operations even on high-dimensional data. This frequently happens - imagine trying to keep track of the counts of words in text data - we'll have thousands of columns, one for each word in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[0:10].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbors\n",
    "\n",
    "Now that we have a way to describe a movie, we want to be able to find other movies that are nearby in feature space.  This is the **nearest neighbors** problem, and Scikit Learn provides a class to handle this.\n",
    "\n",
    "When the `NearestNeighbors` object is fit, it records all of the rows in such a way that it can efficiently look through them to find which is nearest to one queried later.  The `NearestNeighbors` assumes that the closest points are the ones for which the [Minkowski Distance](https://en.wikipedia.org/wiki/Minkowski_distance) is minimized.  That is, if we have two rows of our feature vector $X_{i\\cdot}$ and $X_{j\\cdot}$, it minimizes\n",
    "$$ \\|X_{i\\cdot} - X_{j\\cdot}\\|_p^p = \\sum_k \\left(X_{ik} - X_{jk}\\right)^p $$\n",
    "By default, it computes $p=2$, which is the [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance).\n",
    "\n",
    "Intuitively, we can understand this from our 2-D example above. If we look for the nearest neighbors to *Get Shorty*, we'll be given back other movies that are at 1 on the \"Comedy\" dimension and 0 on the \"Romance\" dimension, since they're closest in the 2-dimensional space. Then (depending on how many neighbors we're looking for), we'd start to get back romantic comedies, and then last, romance movies, since they are farthest away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "rom_nn = NearestNeighbors(n_neighbors=15).fit(rom_com_features)\n",
    "dists, indices = rom_nn.kneighbors(rom_com_features[0], n_neighbors=15)\n",
    "\n",
    "neighbors = rom_coms.iloc[indices[0]].reset_index()\n",
    "neighbors[\"distance\"] = dists[0]\n",
    "neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the intuition, let's run it on the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "nn = NearestNeighbors(n_neighbors=20).fit(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will discuss how to measure performance of these models later, but for now we will just eyeball the results.  We'll pick out two movies as test cases.  *Toy Story* happens to be the first movie in the data set.  *Dr. Strangelove* is a favorite of the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[[0, 737]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the below, the results for *Toy Story* are reasonable.  All of the movies are animated children's movies, and *Toy Story 2* shows up.  But by relying only on the genres, we are unable to select Pixar movies specifically, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists, indices = nn.kneighbors(features[0])\n",
    "df.iloc[indices[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This limitation is more apparent when looking a movies similar to *Dr. Strangelove*.  We get all the *War Comedy* movies, but that doesn't mean that they're similar to *Dr. Strangelove*.  While the author has never seen *The Wackiest Ship in the Army*, he is pretty sure it is of a rather different tone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists, indices = nn.kneighbors(features[737])\n",
    "df.iloc[indices[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag Data\n",
    "\n",
    "The problem is that we're only getting 20 bits of data about each movie from the categories.  We have much more information in the user-applied tags, though, so let's bring that to bear.\n",
    "\n",
    "As a reminder, the tag data is stored one tag application per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df_tags['tag'].value_counts()\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want all tags for each movie, so we group by the movie_id and get a list of tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = df_tags.groupby('movie_id')['tag'].apply(lambda x: x.tolist())\n",
    "all_tags.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equivalent of a SQL `join` statement in Pandas is the `merge()` method.  We specify a left join, to keep movies without any tags, and indicate rows should be matched by index, which is the movie_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = df.merge(all_tags.to_frame(), left_index=True, right_index=True, how='left')\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the same sort of pipeline to one-hot encode the tags.  (It might be better to use an alternative encoding that accounts for the number of times each tag was applied.)  Then, a `FeatureUnion` can join the two one-hot encoded matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_pipe = Pipeline([('encoder', DictEncoder('tag')),\n",
    "                     ('vectorizer', DictVectorizer())])\n",
    "union = FeatureUnion([('categories', cat_pipe),\n",
    "                      ('tags', tag_pipe)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = union.fit_transform(merged)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly 16k features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NearestNeighbors(n_neighbors=20).fit(features)\n",
    "nn.fit(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the tags, we now clearly pick out *Toy Story 2* as the most similar to *Toy Story*.  But the rest seem worse.  We're not getting any of the Pixar movies, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dists, indices = nn.kneighbors(features[0])\n",
    "df.iloc[indices[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem becomes more clear looking at *Dr. Strangelove*.  *The Mouse that Roared* is a good pick, but most of the rest have no tags.  The problem is that the tag space is so large, and so sparsely populated, that even similar movies will have very few overlapping tags.  As a result, movies with no tags end up being the closest to most movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dists, indices = nn.kneighbors(features[737])\n",
    "df.iloc[indices[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensional Reduction\n",
    "\n",
    "We don't actually expect there to be 16k different concepts expressed in the tags.  Rather, several tags may be associated with a single underlying concept.  In the above, we can see tags for *animation* and *cartoon*, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several methods of **dimensional reduction** that attempt to pull out a lower-dimensional structure.  We'll be using Truncated Singular Value Decomposition (SVD), which works well with sparse matrices.  SVD attempts to find orthogonal directions within feature space, along which the feature matrix has the most variation. It's very closely related to Principal Component Analysis (PCA), a very popular dimensionality reduction and clustering technique.\n",
    "\n",
    "In fact, it's so popular that it's worth understanding a bit better. Let's give PCA the same treatment we gave K Nearest Neighbors. We'll look at two tags we know to be semantically identical: \"adapted from: book\" and \"based on a book\". We can imagine that we don't really need two separate dimensions for these tags (really, we don't need two different tags) - we can collapse them to one single dimension conveying the same meaning. \n",
    "\n",
    "Note that we don't see a very dramatic change here in 2 dimensions. PCA (and related decomposition methods) really shines across a high-dimensional feature space with many features, as we'll see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_term = 'adapted from:book'\n",
    "second_term = 'based on a book'\n",
    "\n",
    "feat_pipe = Pipeline([('encoder', DictEncoder(None)),\n",
    "                     ('vectorizer', DictVectorizer()),\n",
    "                     ])\n",
    "features = feat_pipe.fit_transform(all_tags)\n",
    "dv = feat_pipe.steps[1][1]\n",
    "dv.feature_names_\n",
    "indexes = [dv.feature_names_.index(first_term), dv.feature_names_.index(second_term)]\n",
    "term_features = features[:, indexes].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_features = term_features[:, 0]\n",
    "second_features = term_features[:, 1]\n",
    "\n",
    "print \"variance in first dimension: {}\".format(np.var(first_features))\n",
    "print \"variance in second dimension: {}\".format(np.var(second_features))\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "print \"correlation: {}\".format(pearsonr(first_features, second_features )[0])\n",
    "\n",
    "# plot features + random noise to show density\n",
    "first_plot = first_features + np.random.normal(scale=.1, size=features.shape[0])\n",
    "second_plot = second_features + np.random.normal(scale=.1, size=features.shape[0])\n",
    "plt.scatter(first_plot, second_plot)\n",
    "plt.xlabel(first_term)\n",
    "plt.ylabel(second_term)\n",
    "# plot linear regression line\n",
    "fitlinePars = np.polyfit(first_features ,second_features,1)\n",
    "fitline = np.poly1d(fitlinePars)\n",
    "plt.plot(first_features,fitline(first_features), 'r-')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the idea is that if we set this single line as a dimensional basis for our space, we can throw out the second dimension because that first dimension explains most of the variability. Then we just need to remember the distance along the regression line, and how to encode our features into that basis (relative to the line). As it turns out, this is basically a rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def rotMatrix(angle):\n",
    "    return np.array([[np.cos(angle), np.sin(angle)],\n",
    "                    [-np.sin(angle), np.cos(angle)]])\n",
    "\n",
    "# remember this from high school math?\n",
    "angle = -np.arctan(fitlinePars[0])\n",
    "\n",
    "rotated = np.dot(term_features, rotMatrix(angle))\n",
    "rotated_line = np.dot(np.column_stack((first_features,fitline(first_features))), rotMatrix(angle))\n",
    "\n",
    "#rotated = np.array([rotate(x, y, angle - .4) for x, y in zip(first_center, second_center)])\n",
    "noise = np.random.normal(scale=.1, size=rotated.shape[0])\n",
    "noise2 = np.random.normal(scale=.1, size=rotated.shape[0])\n",
    "plt.scatter(rotated[:, 0] + noise, rotated[:, 1] + noise2)\n",
    "\n",
    "plt.plot(rotated_line[:, 0],rotated_line[:, 1], 'r-')\n",
    "\n",
    "print \"new variances: {}, {}\".format(np.var(rotated[:, 0]) , np.var(rotated[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Now, translated into math...\n",
    "Recall that for $X$, an $n \\times p$ feature matrix, we have the [Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) with\n",
    "\n",
    "$$ X = U \\Sigma V^T $$\n",
    "\n",
    "where $U$ is a unitary $n \\times n$ matrix, $\\Sigma$ is a diagonal $n \\times p$ matrix, and $V$ is a unitary $p \\times p$ matrix.  With this decomposition, we can then truncate the diagonal matrix $\\Sigma$ to include only the $m$ dimensions with the most variation.\n",
    "\n",
    "Our choice of $m=100$ seems reasonable, but can be tuned to adjust the sensitivity to the tags. \n",
    "\n",
    "We'll use `TruncatedSVD` instead of PCA for two reasons:\n",
    "1. It plays nice with sparse matrices\n",
    "2. It works well with term count matrices in documents. It is actually also known as LSA (latent semantic analysis) in the NLP world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "tag_svd_pipe = Pipeline([('encoder', DictEncoder('tag')),\n",
    "                     ('vectorizer', DictVectorizer()),\n",
    "                     ('svd', TruncatedSVD(n_components=100))])\n",
    "svd_union = FeatureUnion([('categories', cat_pipe),\n",
    "                      ('tags', tag_svd_pipe)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_features = svd_union.fit_transform(merged)\n",
    "svd_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_nn = NearestNeighbors(n_neighbors=20).fit(svd_features)\n",
    "svd_nn.fit(svd_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we seem to be getting mostly Disney movies for *Toy Story*, which seems reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dists, indices = svd_nn.kneighbors(svd_features[0])\n",
    "df.iloc[indices[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the algorithm finds movies like *The Great Dictator* and *MASH*, which are fairly similar in message to *Dr. Strangelove*.  It also finds *Full Metal Jacket*, another Stanley Kubrick war movie, albeit with a very different tone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists, indices = svd_nn.kneighbors(svd_features[737])\n",
    "df.iloc[indices[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the directions in feature space that the SVD picked out.  Here, we print the top ten tags associated with each dimension of the SVD.  We can see dimensions corresponding to classic movies, sci-fi franchises, comedies, and movies adapted from books.  (Note that the SVD has found that \"based on a book\", \"adapted from:book\", and \"based on book\" are all associated with each other.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[tumey's dvds, imdb top 250, owned, erlend's dvds, dvd, classic, seen more than once, based on a book, r]\",\n",
       " '[r, clearplay, owned, nudity (topless), action, seen at the cinema, movie to see, seen more than once, dvd]',\n",
       " '[70mm, seen more than once, dvd, action, classic, seen at the cinema, futuristmovies.com, franchise, comedy]',\n",
       " '[less than 300 ratings, based on a book, adapted from:book, national film registry, classic, 70mm, afi 100, movie to see, afi 100 (cheers)]',\n",
       " '[less than 300 ratings, stylized, 70mm, atmospheric, tense, criterion, quirky, humorous, seen more than once]',\n",
       " '[based on a book, adapted from:book, stylized, atmospheric, criterion, disturbing, based on book, tense, betamax]',\n",
       " '[comedy, quirky, funny, humorous, romance, seen more than once, satirical, irreverent, classic]',\n",
       " \"[tumey's dvds, less than 300 ratings, owned, bibliothek, based on a book, adapted from:book, movie to see, eric's dvds, own]\",\n",
       " '[movie to see, imdb top 250, owned, national film registry, tense, afi 100, atmospheric, overrated, dvd]',\n",
       " '[nudity (topless), tense, nudity (topless - brief), murder, national film registry, crime, owned, drama, visceral]',\n",
       " \"[movie to see, r, humorous, quirky, tumey's dvds, clearplay, adapted from:book, based on a book, national film registry]\",\n",
       " '[movie to see, 70mm, owned, library, to see, criterion, true story, in netflix queue, dvd-video]',\n",
       " '[70mm, r, drama, bibliothek, clearplay, dvd, own, owned, library]',\n",
       " '[nudity (topless), atmospheric, stylized, seen more than once, dvd, adventure, fantasy, lyrical, reflective]',\n",
       " \"[bibliothek, betamax, seen, movie to see, national film registry, can't remember, classic, dvd-video, action]\",\n",
       " '[betamax, dvd, dvd-video, movie to see, satirical, owned, quirky, futuristmovies.com, nudity (topless)]',\n",
       " '[bibliothek, humorous, 70mm, murder, quirky, national film registry, franchise, nudity (topless), futuristmovies.com]',\n",
       " \"[movie to see, 70mm, dvd, quirky, bibliothek, can't remember, disturbing, erlend's dvds, satirical]\",\n",
       " \"[can't remember, owned, criterion, atmospheric, imdb top 250, nudity (topless - brief), lyrical, reflective, franchise]\",\n",
       " \"[nudity (topless - brief), can't remember, to see, drama, action, erlend's dvds, quirky, humorous, satirical]\",\n",
       " \"[betamax, drama, murder, erlend's dvds, nudity (topless - brief), romance, action, franchise, assassin]\",\n",
       " '[criterion, library, dvd-video, in netflix queue, action, tense, imdb top 250, forceful, crime]',\n",
       " \"[drama, futuristmovies.com, comedy, sci-fi, movie to see, romance, dvd-video, can't remember, space]\",\n",
       " '[stylized, classic, nudity (topless), humorous, disney, adventure, fantasy, action, quirky]',\n",
       " '[library, nudity (topless - brief), futuristmovies.com, sci-fi, directorial debut, disney, national film registry, in netflix queue, animation]',\n",
       " \"[national film registry, seen at the cinema, own, can't remember, library, erlend's dvds, based on a book, action, seen more than once]\",\n",
       " \"[library, dvd, action, murder, can't remember, boring, quirky, atmospheric, comic book]\",\n",
       " \"[seen more than once, own, can't remember, murder, vhs, directorial debut, nudity (topless), movie to see, seen at the cinema]\",\n",
       " \"[70mm, own, crime, eric's dvds, comedy, vhs, comic book, action, stylized]\",\n",
       " '[library vhs, world war ii, history, comic book, based on a book, seen at the cinema, adapted from:comic, true story, disney]',\n",
       " \"[imdb top 250, seen at the cinema, classic, betamax, overrated, vhs, erlend's dvds, atmospheric, tv]\",\n",
       " '[seen more than once, classic, owned, comic book, superhero, adapted from:comic, super-hero, chick flick, alter ego]',\n",
       " '[in netflix queue, directorial debut, overrated, franchise, imdb top 250, dvd-video, need to own, disturbing, boring]',\n",
       " '[classic, adapted from:book, boring, seen at the cinema, overrated, own, disturbing, my movies, nudity (topless - notable)]',\n",
       " '[tense, directorial debut, romance, futuristmovies.com, overrated, oscar (best picture), forceful, oscar (best directing), poignant]',\n",
       " '[imdb top 250, library, disturbing, own, romance, surreal, need to own, based on a book, classic]',\n",
       " '[dvd-video, seen at the cinema, funny, drama, disney, oscar (best picture), adapted from:book, vhs, disney animated feature]',\n",
       " '[vhs, classic, comic book, own, adapted from:comic, superhero, humorous, franchise, library]',\n",
       " '[based on a book, criterion, boring, classic, franchise, remake, divx1, family, overrated]',\n",
       " '[vhs, romance, in netflix queue, disturbing, seen at the cinema, humorous, remake, oscar (best picture), horror]',\n",
       " '[boring, humorous, dvd-video, overrated, reflective, understated, nudity (topless - notable), deliberate, imdb top 250]',\n",
       " \"[action, history, seen more than once, overrated, eric's dvds, satirical, atmospheric, disturbing, religion]\",\n",
       " '[criterion, disturbing, boring, drama, overrated, biting, cynical, bleak, satirical]',\n",
       " \"[in netflix queue, action, oscar (best picture), oscar (best directing), comedy, eric's dvds, stylized, directorial debut, reflective]\",\n",
       " \"[in netflix queue, atmospheric, eric's dvds, romance, franchise, drama, politics, seen at the cinema, satirical]\",\n",
       " '[directorial debut, vhs, romance, action, disney, comedy, reflective, disney animated feature, disturbing]',\n",
       " '[boring, seen more than once, criterion, action, true story, based on a book, my movies, biography, quirky]',\n",
       " \"[eric's dvds, boring, vhs, national film registry, funny, stylized, adapted from:book, quirky, franchise]\",\n",
       " '[nudity (full frontal - notable), japan, fantasy, disturbing, visceral, atmospheric, surreal, cult film, my dvds]',\n",
       " '[nudity (topless - notable), vhs, nudity (full frontal - notable), based on a book, irreverent, drugs, stylized, dvd-video, disney]',\n",
       " '[overrated, deliberate, nudity (full frontal - notable), directorial debut, in netflix queue, national film registry, world war ii, humorous, reflective]',\n",
       " \"[nudity (full frontal - notable), humorous, criterion, politics, nudity (topless - notable), drugs, eric's dvds, imdb top 250, based on a true story]\",\n",
       " \"[remake, murder, seen at the cinema, boring, seen more than once, vhs, classic, afi 100 (laughs), eric's dvds]\",\n",
       " \"[quirky, criterion, boring, drama, nudity (full frontal - notable), dvd-r, seen at the cinema, disney, eric's dvds]\",\n",
       " \"[eric's dvds, comedy, murder, boring, based on a play, dvd-video, atmospheric, adapted from:play, usa]\",\n",
       " \"[nudity (topless - notable), japan, drama, ominous, seen at the cinema, eric's dvds, satirical, classic, true story]\",\n",
       " '[comedy, remake, overrated, library vhs, world war ii, family, based on book, japan, disney]',\n",
       " '[world war ii, funny, nudity (topless - notable), family, criterion, directorial debut, afi 100 (thrills), drugs, history]',\n",
       " '[below r, pg-13, pg13, overrated, bechdel test:fail, netflix, war, stylized, politics]',\n",
       " '[boring, humorous, scope, pg-13, nudity (full frontal - notable), religion, gfei own it, new york city, dvd-r]',\n",
       " '[dvd-r, afi 100 (thrills), remake, netflix, horror, biography, franchise, dvd-ram, cult film]',\n",
       " '[overrated, quirky, true story, biography, bleak, criterion, based on book, fantasy, music]',\n",
       " '[nudity (topless - notable), remake, nudity (full frontal - notable), quirky, netflix, bechdel test:fail, tense, nudity (full frontal), war]',\n",
       " '[world war ii, in netflix queue, remake, below r, humorous, drama, stylized, based on a play, netflix]',\n",
       " \"[japan, poignant, afi 100 (laughs), below r, directorial debut, bechdel test:fail, drugs, eric's dvds, new york city]\",\n",
       " \"[drugs, my movies, dvd-r, sven's to see list, not corv lib, oscar (best cinematography), eric's dvds, romance, deliberate]\",\n",
       " \"[japan, nudity (full frontal - notable), thriller, quirky, religion, eric's dvds, crime, new york city, world war ii]\",\n",
       " \"[netflix, sven's to see list, family, my movies, funny, deliberate, library vhs, need to own, based on a play]\",\n",
       " \"[nudity (full frontal - notable), my movies, sven's to see list, world war ii, pg-13, ummarti2006, based on a play, dvd-r, adapted from:play]\",\n",
       " '[world war ii, not corv lib, divx1, nudity (full frontal), drugs, dreamlike, music, atmospheric, racism]',\n",
       " '[crime, disturbing, politics, deliberate, remake, aliens, rousing, afi 100 (cheers), want]',\n",
       " \"[below r, sven's to see list, library vhs, nudity (rear), potential oscar nom, oscar (best cinematography), oscar nom 2007, imdb top 250, sequel]\",\n",
       " '[crime, world war ii, based on book, need to own, clv, poignant, talky, overrated, true story]',\n",
       " \"[divx, sven's to see list, afi 100 (cheers), history, library vhs, crime, netflix, fantasy, surreal]\",\n",
       " '[dvd-r, not corv lib, disturbing, humorous, dvd-ram, new york city, divx, talky, comedy]',\n",
       " '[gay, netflix, 3, humorous, library vhs, interesting, pg-13, psychology, good]',\n",
       " \"[politics, gay, ummarti2006, sven's to see list, japan, dvd-video, tv, boring, want to see again]\",\n",
       " '[history, new york city, time travel, teen, need to own, sequel, vhs, sweeping, literate]',\n",
       " '[hilarious, clv, irreverent, gay, nudity (rear), based on a tv show, afi 100 (cheers), gritty, murder]',\n",
       " '[want to see again, dvd-r, visceral, oscar (best cinematography), want, afi 100 (laughs), divx, not corv lib, corvallis library]',\n",
       " '[sequel, lyrical, on dvr, gothic, scope, reflective, easily confused with other movie(s) (title), adventure, my dvds]',\n",
       " \"[oscar (best actress), sven's to see list, afi 100 (cheers), own, want to see again, on computer, divx, violence, quirky]\",\n",
       " '[pg-13, not corv lib, bleak, my movies, whimsical, bittersweet, violence, heartwarming, religion]',\n",
       " '[bechdel test:fail, violence, r, talky, horror, afi 100 (thrills), my movies, oscar (best cinematography), scope]',\n",
       " '[not corv lib, on dvr, japan, lyrical, drugs, tense, remake, hw drama, funny]',\n",
       " '[cult film, sequel, corvallis library, campy, thriller, family, silly, based on a tv show, my movies]',\n",
       " \"[sven's to see list, on dvr, religion, divx, seen at the cinema, sequel, nudity (rear), high school, avi]\",\n",
       " '[gfei own it, high school, not corv lib, afi 100 (cheers), easily confused with other movie(s) (title), teen, drugs, black and white, poignant]',\n",
       " \"[dvd-r, teen, sven's to see list, war, time travel, nudity (full frontal), based on book, high school, afi 100 (laughs)]\",\n",
       " \"[on dvr, not corv lib, afi 100 (thrills), dreamlike, new york city, disturbing, pg13, bleak, sven's to see list]\",\n",
       " '[easily confused with other movie(s) (title), remake, not corv lib, oscar (best actress), oscar (best actor), fantasy, religion, dvd-r, afi 100 (laughs)]',\n",
       " '[potential oscar nom, gfei own it, oscar nom 2007, crime, ummarti2006, biography, watched 2007, library vhs, want to see again]',\n",
       " \"[sven's to see list, true story, lavish, my movies, on computer, dvd-r, gay, afi 100 (movie quotes), religion]\",\n",
       " '[my movies, true story, based on a tv show, dvd-r, motorcycle, clearplay, 3, cynical, tv]',\n",
       " '[not corv lib, based on a tv show, easily confused with other movie(s) (title), nudity (full frontal), musical, lavish, watched 2006, clv, understated]',\n",
       " '[on dvr, nudity (full frontal), war, clearplay, pg-13, world war ii, not corv lib, pg13, oscar (best actress)]',\n",
       " '[family, drugs, clearplay, adventure, based on a tv show, sad, pg-13, ominous, twist ending]',\n",
       " '[need to own, clearplay, history, sports, horror, underrated, beautiful, below r, visceral]',\n",
       " '[based on a tv show, new york city, history, afi 100 (laughs), watched 2006, nudity (rear), new york, imdb top 250, cult film]',\n",
       " '[dark comedy, black comedy, nudity (full frontal - notable), family, not corv lib, pg-13, oscar (best actor), awesome, need to buy]']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = tag_svd_pipe.named_steps['svd']\n",
    "vect = tag_svd_pipe.named_steps['vectorizer']\n",
    "[(\"[\" + \", \".join([vect.feature_names_[i] for i in c.argsort()[:-10:-1]]) + \"]\")\n",
    " for c in svd.components_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another alternative: NMF\n",
    "\n",
    "When our feature matrix contains non-negative features, we can use another extremely similar algorithm, Non-Negative Matrix Factorization (NMF), to reduce our feature space in a nicely explainable way. \n",
    "\n",
    "If we absorb the $\\Sigma_m$ into $U$ and $V^T$ as $W$ and $H$, we could rewrite this as \n",
    "$$ \\min_{W, H} \\| X - W H \\|_2 $$\n",
    "where $W$ is a $n \\times m$ matrix and $H$ is a $m \\times p$ matrix.  If $X$ has only non-negative values, we might want $W$ and $H$ to have non-negative values as well.  Hence, Non-negative Matrix Factorization is just\n",
    "$$ \\min_{W \\ge 0, H \\ge 0} \\| X - W H \\|_2 $$\n",
    "when $X \\ge 0$ (here, we use $X \\ge 0$ to mean that each element of $X$ has non-negative values).  While PCA gives you a more accurate low-dimensional representation, NMF can give a more interpretable results since the values are non-negative.\n",
    "\n",
    "This is often true of text and image data, where words and pixel value features are strictly \"positive\" and we'd like our lower-dimensional representations to also be positive. For example, it can be useful to think of text data this way:\n",
    "\n",
    "![Annotated Wikipedia NMF diagram](nmf_example.png)\n",
    "\n",
    "Just like in PCA, we have to choose the number of \"archetypes\" $m \\ll n,p$ in order to compress the data, but the larger $m$, the more variance is retained.\n",
    "\n",
    "In scikit, the matrix $H$ is called the `components_` while $W$ is the value returned from `.transform`.\n",
    "\n",
    "Below, we are analyzing images of faces.  Each row is a greyscale 64 x 64 dimensional image so the feature space is 4096 dimensional!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "         metric_params=None, n_jobs=1, n_neighbors=20, p=2, radius=1.0)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "tag_nmf_pipe = Pipeline([('encoder', DictEncoder('tag')),\n",
    "                     ('vectorizer', DictVectorizer()),\n",
    "                     ('nmf', NMF(n_components=100))])\n",
    "nmf_union = FeatureUnion([('categories', cat_pipe),\n",
    "                      ('tags', tag_nmf_pipe)])\n",
    "\n",
    "nmf_features = nmf_union.fit_transform(merged)\n",
    "nmf_nn = NearestNeighbors(n_neighbors=20).fit(nmf_features)\n",
    "nmf_nn.fit(nmf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>General, The (1927)</td>\n",
       "      <td>1927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>McHale's Navy (1997)</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>How I Won the War (1967)</td>\n",
       "      <td>1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7159</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Two Men Went to War (2003)</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>In the Army Now (1994)</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5801</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Russians Are Coming, the Russians Are Coming, ...</td>\n",
       "      <td>1966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63810</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>When Willie Comes Marching Home (1950)</td>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9001</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Wackiest Ship in the Army, The (1960)</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Canadian Bacon (1995)</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4802</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Operation Petticoat (1959)</td>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5789</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>All the Queen's Men (2001)</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4349</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Catch-22 (1970)</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6561</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Mouse That Roared, The (1959)</td>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7292</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Best Defense (1984)</td>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7104</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>1941 (1979)</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>And the Ship Sails On (E la nave va) (1983)</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Bananas (1971)</td>\n",
       "      <td>1971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55830</th>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>Be Kind Rewind (2008)</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Dr. Strangelove or: How I Learned to Stop Worr...</td>\n",
       "      <td>1964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25752</th>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>Freshman, The (1925)</td>\n",
       "      <td>1925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          categories                                              title  year\n",
       "id                                                                           \n",
       "3022   [Comedy, War]                                General, The (1927)  1927\n",
       "1445   [Comedy, War]                               McHale's Navy (1997)  1997\n",
       "3049   [Comedy, War]                           How I Won the War (1967)  1967\n",
       "7159   [Comedy, War]                         Two Men Went to War (2003)  2003\n",
       "473    [Comedy, War]                             In the Army Now (1994)  1994\n",
       "5801   [Comedy, War]  Russians Are Coming, the Russians Are Coming, ...  1966\n",
       "63810  [Comedy, War]             When Willie Comes Marching Home (1950)  1950\n",
       "9001   [Comedy, War]              Wackiest Ship in the Army, The (1960)  1960\n",
       "157    [Comedy, War]                              Canadian Bacon (1995)  1995\n",
       "4802   [Comedy, War]                         Operation Petticoat (1959)  1959\n",
       "5789   [Comedy, War]                         All the Queen's Men (2001)  2001\n",
       "4349   [Comedy, War]                                    Catch-22 (1970)  1970\n",
       "6561   [Comedy, War]                      Mouse That Roared, The (1959)  1959\n",
       "7292   [Comedy, War]                                Best Defense (1984)  1984\n",
       "7104   [Comedy, War]                                        1941 (1979)  1979\n",
       "2897   [Comedy, War]        And the Ship Sails On (E la nave va) (1983)  1983\n",
       "1078   [Comedy, War]                                     Bananas (1971)  1971\n",
       "55830       [Comedy]                              Be Kind Rewind (2008)  2008\n",
       "750    [Comedy, War]  Dr. Strangelove or: How I Learned to Stop Worr...  1964\n",
       "25752       [Comedy]                               Freshman, The (1925)  1925"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists, indices = nmf_nn.kneighbors(svd_features[737])\n",
    "df.iloc[indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3114</th>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>Toy Story 2 (1999)</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4016</th>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>Emperor's New Groove, The (2000)</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26662</th>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>Kiki's Delivery Service (Majo no takkybin) (1...</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>Antz (1998)</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45074</th>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>Wild, The (2006)</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47124</th>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>Ant Bully, The (2006)</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>Adventures of Rocky and Bullwinkle, The (2000)</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53121</th>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>Shrek the Third (2007)</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33615</th>\n",
       "      <td>[Adventure, Animation, Children, Comedy]</td>\n",
       "      <td>Madagascar (2005)</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4886</th>\n",
       "      <td>[Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>Monsters, Inc. (2001)</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2355</th>\n",
       "      <td>[Adventure, Animation, Children, Comedy]</td>\n",
       "      <td>Bug's Life, A (1998)</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fanta...</td>\n",
       "      <td>Return of Jafar, The (1994)</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6536</th>\n",
       "      <td>[Adventure, Animation, Children, Comedy]</td>\n",
       "      <td>Sinbad: Legend of the Seven Seas (2003)</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5218</th>\n",
       "      <td>[Adventure, Animation, Children, Comedy]</td>\n",
       "      <td>Ice Age (2002)</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5672</th>\n",
       "      <td>[Adventure, Animation, Children, Fantasy]</td>\n",
       "      <td>Pokemon 4 Ever (a.k.a. Pokmon 4: The Movie) (...</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4990</th>\n",
       "      <td>[Adventure, Animation, Children, Comedy]</td>\n",
       "      <td>Jimmy Neutron: Boy Genius (2001)</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fanta...</td>\n",
       "      <td>Space Jam (1996)</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2141</th>\n",
       "      <td>[Adventure, Animation, Children, Comedy]</td>\n",
       "      <td>American Tail, An (1986)</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>[Adventure, Animation, Children, Fantasy]</td>\n",
       "      <td>We're Back! A Dinosaur's Story (1993)</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              categories  \\\n",
       "id                                                         \n",
       "1      [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "3114   [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "4016   [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "26662  [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "2294   [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "45074  [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "47124  [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "3754   [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "53121  [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "33615           [Adventure, Animation, Children, Comedy]   \n",
       "4886              [Animation, Children, Comedy, Fantasy]   \n",
       "2355            [Adventure, Animation, Children, Comedy]   \n",
       "2092   [Adventure, Animation, Children, Comedy, Fanta...   \n",
       "6536            [Adventure, Animation, Children, Comedy]   \n",
       "5218            [Adventure, Animation, Children, Comedy]   \n",
       "5672           [Adventure, Animation, Children, Fantasy]   \n",
       "4990            [Adventure, Animation, Children, Comedy]   \n",
       "673    [Adventure, Animation, Children, Comedy, Fanta...   \n",
       "2141            [Adventure, Animation, Children, Comedy]   \n",
       "3400           [Adventure, Animation, Children, Fantasy]   \n",
       "\n",
       "                                                   title  year  \n",
       "id                                                              \n",
       "1                                       Toy Story (1995)  1995  \n",
       "3114                                  Toy Story 2 (1999)  1999  \n",
       "4016                    Emperor's New Groove, The (2000)  2000  \n",
       "26662  Kiki's Delivery Service (Majo no takkybin) (1...  1989  \n",
       "2294                                         Antz (1998)  1998  \n",
       "45074                                   Wild, The (2006)  2006  \n",
       "47124                              Ant Bully, The (2006)  2006  \n",
       "3754      Adventures of Rocky and Bullwinkle, The (2000)  2000  \n",
       "53121                             Shrek the Third (2007)  2007  \n",
       "33615                                  Madagascar (2005)  2005  \n",
       "4886                               Monsters, Inc. (2001)  2001  \n",
       "2355                                Bug's Life, A (1998)  1998  \n",
       "2092                         Return of Jafar, The (1994)  1994  \n",
       "6536             Sinbad: Legend of the Seven Seas (2003)  2003  \n",
       "5218                                      Ice Age (2002)  2002  \n",
       "5672   Pokemon 4 Ever (a.k.a. Pokmon 4: The Movie) (...  2002  \n",
       "4990                    Jimmy Neutron: Boy Genius (2001)  2001  \n",
       "673                                     Space Jam (1996)  1996  \n",
       "2141                            American Tail, An (1986)  1986  \n",
       "3400               We're Back! A Dinosaur's Story (1993)  1993  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists, indices = nmf_nn.kneighbors(nmf_features[0])\n",
    "df.iloc[indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf = tag_nmf_pipe.named_steps['nmf']\n",
    "vect = tag_nmf_pipe.named_steps['vectorizer']\n",
    "[(\"[\" + \", \".join([vect.feature_names_[i] for i in c.argsort()[:-10:-1]]) + \"]\")\n",
    " for c in nmf.components_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Brief Aside: The Hashing Trick\n",
    "\n",
    "In many NLP and text-based analysis applications, one-hot encoding each word into its own column can prove extremely expensive, and doesn't boost a model's performance. This is typically known as the *curse of dimensionality*. Instead, we can attempt to take advantage of two properties:\n",
    "\n",
    "1. The fact that most English words (and in this case, tags) are used infrequently. This is known as [zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law).\n",
    "2. The structured randomness of hash functions.\n",
    "\n",
    "Basically, we can use a [hash function](https://en.wikipedia.org/wiki/Hash_function) to map our words to one of `N` bins with uniform randomness. Given that if we point at two random words in the dictionary, it's likely that they will be common words, we get the property that only very uncommon words will collide in the dimensionally reduced feature space.\n",
    "\n",
    "The hashing trick isn't the best candidate here, but we still demonstrate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sbrice/miniconda3/envs/strata/lib/python2.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1105b8e10>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFXCAYAAAB6G51YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ef82910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.stats\n",
    "import numpy as np\n",
    "counts = df_tags[\"tag\"].value_counts().sort_values(ascending=False).reset_index()[\"tag\"]\n",
    "ranks = counts.index + 1\n",
    "plt.plot(np.log(ranks), np.log(counts), label=\"our counts\")\n",
    "plt.plot(np.log(range(1000)), np.log(10000* scipy.stats.zipf.pmf(range(1000), 1.1)), 'r-', label=\"zipfian distribution\")\n",
    "plt.title(\"This is why the hashing trick is maybe a bad idea\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Dr. Strangelove or: How I Learned to Stop Worr...</td>\n",
       "      <td>1964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9001</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Wackiest Ship in the Army, The (1960)</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7159</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Two Men Went to War (2003)</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>How I Won the War (1967)</td>\n",
       "      <td>1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63810</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>When Willie Comes Marching Home (1950)</td>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5801</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Russians Are Coming, the Russians Are Coming, ...</td>\n",
       "      <td>1966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>In the Army Now (1994)</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6561</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Mouse That Roared, The (1959)</td>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>General, The (1927)</td>\n",
       "      <td>1927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Bananas (1971)</td>\n",
       "      <td>1971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4349</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Catch-22 (1970)</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7104</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>1941 (1979)</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>And the Ship Sails On (E la nave va) (1983)</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>McHale's Navy (1997)</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Stripes (1981)</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7292</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Best Defense (1984)</td>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5270</th>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>Teddy Bears' Picnic (2001)</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5307</th>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>Taking Care of Business (1990)</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5326</th>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>Frank McKlusky, C.I. (2002)</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5360</th>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>Survivors, The (1983)</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          categories                                              title  year\n",
       "id                                                                           \n",
       "750    [Comedy, War]  Dr. Strangelove or: How I Learned to Stop Worr...  1964\n",
       "9001   [Comedy, War]              Wackiest Ship in the Army, The (1960)  1960\n",
       "7159   [Comedy, War]                         Two Men Went to War (2003)  2003\n",
       "3049   [Comedy, War]                           How I Won the War (1967)  1967\n",
       "63810  [Comedy, War]             When Willie Comes Marching Home (1950)  1950\n",
       "5801   [Comedy, War]  Russians Are Coming, the Russians Are Coming, ...  1966\n",
       "473    [Comedy, War]                             In the Army Now (1994)  1994\n",
       "6561   [Comedy, War]                      Mouse That Roared, The (1959)  1959\n",
       "3022   [Comedy, War]                                General, The (1927)  1927\n",
       "1078   [Comedy, War]                                     Bananas (1971)  1971\n",
       "4349   [Comedy, War]                                    Catch-22 (1970)  1970\n",
       "7104   [Comedy, War]                                        1941 (1979)  1979\n",
       "2897   [Comedy, War]        And the Ship Sails On (E la nave va) (1983)  1983\n",
       "1445   [Comedy, War]                               McHale's Navy (1997)  1997\n",
       "1663   [Comedy, War]                                     Stripes (1981)  1981\n",
       "7292   [Comedy, War]                                Best Defense (1984)  1984\n",
       "5270        [Comedy]                         Teddy Bears' Picnic (2001)  2001\n",
       "5307        [Comedy]                     Taking Care of Business (1990)  1990\n",
       "5326        [Comedy]                        Frank McKlusky, C.I. (2002)  2002\n",
       "5360        [Comedy]                              Survivors, The (1983)  1983"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "class StringEncoder(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, col):\n",
    "        self.col = col\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):            \n",
    "        return X[self.col].apply(lambda x: (\" \".join(x) if isinstance(x, list) else \"\"))\n",
    "\n",
    "tag_pipe = Pipeline([('encoder', StringEncoder('tag')),\n",
    "                     ('vectorizer', HashingVectorizer(n_features=2000, non_negative=True))])\n",
    "\n",
    "union = FeatureUnion([('categories', cat_pipe),\n",
    "                      ('tags', tag_pipe)])\n",
    "\n",
    "features = union.fit_transform(merged)\n",
    "nn = NearestNeighbors(n_neighbors=20).fit(features)\n",
    "nn.fit(features)\n",
    "dists, indices = nn.kneighbors(features[737])\n",
    "df.iloc[indices[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great, but we'll see that NMF is equally as performant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Dr. Strangelove or: How I Learned to Stop Worr...</td>\n",
       "      <td>1964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6561</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Mouse That Roared, The (1959)</td>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Bananas (1971)</td>\n",
       "      <td>1971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5789</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>All the Queen's Men (2001)</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>How I Won the War (1967)</td>\n",
       "      <td>1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7159</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Two Men Went to War (2003)</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5801</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Russians Are Coming, the Russians Are Coming, ...</td>\n",
       "      <td>1966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>In the Army Now (1994)</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9001</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Wackiest Ship in the Army, The (1960)</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63810</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>When Willie Comes Marching Home (1950)</td>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Canadian Bacon (1995)</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>General, The (1927)</td>\n",
       "      <td>1927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Stripes (1981)</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4802</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Operation Petticoat (1959)</td>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>And the Ship Sails On (E la nave va) (1983)</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>McHale's Navy (1997)</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4349</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Catch-22 (1970)</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7104</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>1941 (1979)</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7292</th>\n",
       "      <td>[Comedy, War]</td>\n",
       "      <td>Best Defense (1984)</td>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>Liar Liar (1997)</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          categories                                              title  year\n",
       "id                                                                           \n",
       "750    [Comedy, War]  Dr. Strangelove or: How I Learned to Stop Worr...  1964\n",
       "6561   [Comedy, War]                      Mouse That Roared, The (1959)  1959\n",
       "1078   [Comedy, War]                                     Bananas (1971)  1971\n",
       "5789   [Comedy, War]                         All the Queen's Men (2001)  2001\n",
       "3049   [Comedy, War]                           How I Won the War (1967)  1967\n",
       "7159   [Comedy, War]                         Two Men Went to War (2003)  2003\n",
       "5801   [Comedy, War]  Russians Are Coming, the Russians Are Coming, ...  1966\n",
       "473    [Comedy, War]                             In the Army Now (1994)  1994\n",
       "9001   [Comedy, War]              Wackiest Ship in the Army, The (1960)  1960\n",
       "63810  [Comedy, War]             When Willie Comes Marching Home (1950)  1950\n",
       "157    [Comedy, War]                              Canadian Bacon (1995)  1995\n",
       "3022   [Comedy, War]                                General, The (1927)  1927\n",
       "1663   [Comedy, War]                                     Stripes (1981)  1981\n",
       "4802   [Comedy, War]                         Operation Petticoat (1959)  1959\n",
       "2897   [Comedy, War]        And the Ship Sails On (E la nave va) (1983)  1983\n",
       "1445   [Comedy, War]                               McHale's Navy (1997)  1997\n",
       "4349   [Comedy, War]                                    Catch-22 (1970)  1970\n",
       "7104   [Comedy, War]                                        1941 (1979)  1979\n",
       "7292   [Comedy, War]                                Best Defense (1984)  1984\n",
       "1485        [Comedy]                                   Liar Liar (1997)  1997"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_nmf_pipe = Pipeline([('encoder', StringEncoder('tag')),\n",
    "                     ('vectorizer', HashingVectorizer(n_features=4000, non_negative=True)),\n",
    "                     ('nmf', NMF(n_components=100))])\n",
    "nmf_union = FeatureUnion([('categories', cat_pipe),\n",
    "                      ('tags', tag_nmf_pipe)])\n",
    "\n",
    "features = nmf_union.fit_transform(merged)\n",
    "nn = NearestNeighbors(n_neighbors=20).fit(features)\n",
    "nn.fit(features)\n",
    "dists, indices = nn.kneighbors(features[737])\n",
    "df.iloc[indices[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation for a User\n",
    "\n",
    "The models so far seem to do a pretty good job of suggesting movies similar to a particular movie.  Sometimes, this is all that is needed.  If we notice a user buying a particular movie, we could use this to suggest similar movies they might like.\n",
    "\n",
    "Often, however, we wish to suggest a movie to a user.  One approach would be to calculate an \"average movie\" that a particular user enjoyed, and then use the nearest neighbors approach to find movies similar to that one.\n",
    "\n",
    "To help us judge, we'll add the movie titles to the ratings table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ratings_title = df_ratings.merge(df[['title']], left_on='movie_id', right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll look at user 9689, who seems to be a fan of horror."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>user_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1320721</th>\n",
       "      <td>2367</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9689</td>\n",
       "      <td>King Kong (1976)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320715</th>\n",
       "      <td>1982</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9689</td>\n",
       "      <td>Halloween (1978)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320709</th>\n",
       "      <td>1345</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9689</td>\n",
       "      <td>Carrie (1976)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320708</th>\n",
       "      <td>1339</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9689</td>\n",
       "      <td>Dracula (Bram Stoker's Dracula) (1992)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320706</th>\n",
       "      <td>1258</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9689</td>\n",
       "      <td>Shining, The (1980)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320705</th>\n",
       "      <td>1219</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9689</td>\n",
       "      <td>Psycho (1960)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320717</th>\n",
       "      <td>1994</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9689</td>\n",
       "      <td>Poltergeist (1982)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320713</th>\n",
       "      <td>1407</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9689</td>\n",
       "      <td>Scream (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320718</th>\n",
       "      <td>2003</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9689</td>\n",
       "      <td>Gremlins (1984)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320716</th>\n",
       "      <td>1991</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9689</td>\n",
       "      <td>Child's Play (1988)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320711</th>\n",
       "      <td>1387</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9689</td>\n",
       "      <td>Jaws (1975)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320712</th>\n",
       "      <td>1388</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9689</td>\n",
       "      <td>Jaws 2 (1978)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320720</th>\n",
       "      <td>2160</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9689</td>\n",
       "      <td>Rosemary's Baby (1968)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320722</th>\n",
       "      <td>2455</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9689</td>\n",
       "      <td>Fly, The (1986)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320704</th>\n",
       "      <td>1214</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9689</td>\n",
       "      <td>Alien (1979)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320710</th>\n",
       "      <td>1347</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9689</td>\n",
       "      <td>Nightmare on Elm Street, A (1984)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320702</th>\n",
       "      <td>382</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9689</td>\n",
       "      <td>Wolf (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320701</th>\n",
       "      <td>366</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9689</td>\n",
       "      <td>Wes Craven's New Nightmare (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320703</th>\n",
       "      <td>1128</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9689</td>\n",
       "      <td>Fog, The (1980)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320719</th>\n",
       "      <td>2004</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9689</td>\n",
       "      <td>Gremlins 2: The New Batch (1990)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320714</th>\n",
       "      <td>1970</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9689</td>\n",
       "      <td>Nightmare on Elm Street 3: Dream Warriors, A (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320707</th>\n",
       "      <td>1320</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9689</td>\n",
       "      <td>Alien (1992)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         movie_id  rating  user_id  \\\n",
       "1320721      2367     5.0     9689   \n",
       "1320715      1982     5.0     9689   \n",
       "1320709      1345     5.0     9689   \n",
       "1320708      1339     5.0     9689   \n",
       "1320706      1258     5.0     9689   \n",
       "1320705      1219     4.0     9689   \n",
       "1320717      1994     4.0     9689   \n",
       "1320713      1407     4.0     9689   \n",
       "1320718      2003     4.0     9689   \n",
       "1320716      1991     4.0     9689   \n",
       "1320711      1387     4.0     9689   \n",
       "1320712      1388     3.0     9689   \n",
       "1320720      2160     3.0     9689   \n",
       "1320722      2455     3.0     9689   \n",
       "1320704      1214     3.0     9689   \n",
       "1320710      1347     2.0     9689   \n",
       "1320702       382     2.0     9689   \n",
       "1320701       366     2.0     9689   \n",
       "1320703      1128     2.0     9689   \n",
       "1320719      2004     2.0     9689   \n",
       "1320714      1970     1.0     9689   \n",
       "1320707      1320     1.0     9689   \n",
       "\n",
       "                                                     title  \n",
       "1320721                                   King Kong (1976)  \n",
       "1320715                                   Halloween (1978)  \n",
       "1320709                                      Carrie (1976)  \n",
       "1320708             Dracula (Bram Stoker's Dracula) (1992)  \n",
       "1320706                                Shining, The (1980)  \n",
       "1320705                                      Psycho (1960)  \n",
       "1320717                                 Poltergeist (1982)  \n",
       "1320713                                      Scream (1996)  \n",
       "1320718                                    Gremlins (1984)  \n",
       "1320716                                Child's Play (1988)  \n",
       "1320711                                        Jaws (1975)  \n",
       "1320712                                      Jaws 2 (1978)  \n",
       "1320720                             Rosemary's Baby (1968)  \n",
       "1320722                                    Fly, The (1986)  \n",
       "1320704                                       Alien (1979)  \n",
       "1320710                  Nightmare on Elm Street, A (1984)  \n",
       "1320702                                        Wolf (1994)  \n",
       "1320701                  Wes Craven's New Nightmare (1994)  \n",
       "1320703                                    Fog, The (1980)  \n",
       "1320719                   Gremlins 2: The New Batch (1990)  \n",
       "1320714  Nightmare on Elm Street 3: Dream Warriors, A (...  \n",
       "1320707                                      Alien (1992)  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uid = 9689\n",
    "df_ratings_title[df_ratings_title.user_id == uid].sort_values('rating', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll compute a weighted average movie, using the scores given as ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_locs = df_ratings[df_ratings.user_id == uid]['movie_id'].apply(lambda x: df.index.get_loc(x))\n",
    "scaled_ratings = df_ratings[df_ratings.user_id == uid]['rating'] * 0.2\n",
    "weighted_avg_movie = scaled_ratings.values.reshape(1,-1).dot(nmf_features[m_locs,:].toarray()) / len(scaled_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as expected, we get a bunch of horror movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>[Horror]</td>\n",
       "      <td>Jeepers Creepers (2001)</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7886</th>\n",
       "      <td>[Horror]</td>\n",
       "      <td>Countess Dracula (1972)</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6500</th>\n",
       "      <td>[Horror]</td>\n",
       "      <td>Satanic Rites of Dracula, The (1974)</td>\n",
       "      <td>1974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6913</th>\n",
       "      <td>[Horror]</td>\n",
       "      <td>Frankenstein and the Monster from Hell (1974)</td>\n",
       "      <td>1974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4518</th>\n",
       "      <td>[Horror]</td>\n",
       "      <td>Lair of the White Worm, The (1988)</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2649</th>\n",
       "      <td>[Horror]</td>\n",
       "      <td>Son of Frankenstein (1939)</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2652</th>\n",
       "      <td>[Horror]</td>\n",
       "      <td>Curse of Frankenstein, The (1957)</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2868</th>\n",
       "      <td>[Horror]</td>\n",
       "      <td>Fright Night Part II (1989)</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>[Horror]</td>\n",
       "      <td>Nosferatu in Venice (a.k.a. Vampire in Venice)...</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4541</th>\n",
       "      <td>[Horror]</td>\n",
       "      <td>Serpent and the Rainbow, The (1988)</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5195</th>\n",
       "      <td>[Horror]</td>\n",
       "      <td>Zombie Lake (Le Lac des morts vivants) (1981)</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7202</th>\n",
       "      <td>[Horror]</td>\n",
       "      <td>Beyond Re-Animator (2003)</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>[Horror]</td>\n",
       "      <td>Howling II: Your Sister Is a Werewolf (1985)</td>\n",
       "      <td>1985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5555</th>\n",
       "      <td>[Horror]</td>\n",
       "      <td>Wolfen (1981)</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4514</th>\n",
       "      <td>[Horror]</td>\n",
       "      <td>Howling IV: The Original Nightmare (1988)</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3205</th>\n",
       "      <td>[Horror]</td>\n",
       "      <td>Black Sunday (Maschera del demonio, La) (1960)</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7120</th>\n",
       "      <td>[Horror]</td>\n",
       "      <td>DarkWolf (2003)</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5766</th>\n",
       "      <td>[Horror]</td>\n",
       "      <td>Madman (1981)</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5844</th>\n",
       "      <td>[Horror]</td>\n",
       "      <td>Prowler, The (a.k.a. Rosemary's Killer) (a.k.a...</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8854</th>\n",
       "      <td>[Horror]</td>\n",
       "      <td>Night of the Demons (1988)</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     categories                                              title  year\n",
       "id                                                                      \n",
       "4744   [Horror]                            Jeepers Creepers (2001)  2001\n",
       "7886   [Horror]                            Countess Dracula (1972)  1972\n",
       "6500   [Horror]               Satanic Rites of Dracula, The (1974)  1974\n",
       "6913   [Horror]      Frankenstein and the Monster from Hell (1974)  1974\n",
       "4518   [Horror]                 Lair of the White Worm, The (1988)  1988\n",
       "2649   [Horror]                         Son of Frankenstein (1939)  1939\n",
       "2652   [Horror]                  Curse of Frankenstein, The (1957)  1957\n",
       "2868   [Horror]                        Fright Night Part II (1989)  1989\n",
       "1349   [Horror]  Nosferatu in Venice (a.k.a. Vampire in Venice)...  1986\n",
       "4541   [Horror]                Serpent and the Rainbow, The (1988)  1988\n",
       "5195   [Horror]      Zombie Lake (Le Lac des morts vivants) (1981)  1981\n",
       "7202   [Horror]                          Beyond Re-Animator (2003)  2003\n",
       "2655   [Horror]       Howling II: Your Sister Is a Werewolf (1985)  1985\n",
       "5555   [Horror]                                      Wolfen (1981)  1981\n",
       "4514   [Horror]          Howling IV: The Original Nightmare (1988)  1988\n",
       "3205   [Horror]     Black Sunday (Maschera del demonio, La) (1960)  1960\n",
       "7120   [Horror]                                    DarkWolf (2003)  2003\n",
       "5766   [Horror]                                      Madman (1981)  1981\n",
       "5844   [Horror]  Prowler, The (a.k.a. Rosemary's Killer) (a.k.a...  1981\n",
       "8854   [Horror]                         Night of the Demons (1988)  1988"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists, indices = nmf_nn.kneighbors(weighted_avg_movie)\n",
    "df.iloc[indices[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cooperative Learning\n",
    "\n",
    "Content-based recommendation systems work well to find items similar to those a user already likes.  To help with that, we turn to a collaborative learning model.  Instead of finding movies similar to movies a user liked, we will find *users* similar to a given user, and then find the movies that they rated highly.\n",
    "\n",
    "How do we describe users?  We could use demographic information, but we have better set of features handy: the users' ratings.  Two users are similar if they have rated movies similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "by_user_ratings = df_ratings.groupby('user_id').apply(\n",
    "    lambda items: {i[1]: i[2] for i in items.itertuples()}) # 0 is index\n",
    "features = DictVectorizer().fit_transform(by_user_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we have been using the default [Euclidean metric](https://en.wikipedia.org/wiki/Euclidean_distance) in the nearest neighbors calculation:\n",
    "\n",
    "$$ d(x, y) =  \\left| x - y \\right|^2 \\ . $$\n",
    "\n",
    "For users, we will instead use the [cosine metric](https://en.wikipedia.org/wiki/Cosine_similarity),\n",
    "\n",
    "$$ d(x, y) = 1 - \\frac{ x\\cdot y}{|x|\\ |y|} \\ , $$\n",
    "\n",
    "which cares about angles between vectors in feature space.  This dependence on angle only lessens the effect of users using different scales to rate the movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = NearestNeighbors(n_neighbors=20, metric='cosine', algorithm='brute').fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419 recommendations\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eXistenZ (1999)</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>If.... (1968)</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Go (1999)</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gladiator (2000)</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Firestarter (1984)</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mean  count\n",
       "title                          \n",
       "eXistenZ (1999)      5.0      1\n",
       "If.... (1968)        5.0      1\n",
       "Go (1999)            5.0      1\n",
       "Gladiator (2000)     5.0      2\n",
       "Firestarter (1984)   5.0      1"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_row = features[by_user_ratings.index.get_loc(uid), :]\n",
    "dists, indices = nn.kneighbors(user_row)\n",
    "neighbors = [by_user_ratings.index[i] for i in indices[0]][1:] # value at index 0 is our user themself\n",
    "ratings_grp = df_ratings_title[df_ratings_title['user_id'].isin(neighbors)] \\\n",
    "    .groupby('title')['rating']\n",
    "print \"{} recommendations\".format(len(ratings_grp))\n",
    "ratings_grp.agg(['mean', 'count']).sort_values('mean', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the counts - a familiar problem..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. At the beginning of this notebook, we discussed Bayesian smoothing. Use Bayesian smoothing to appropriately sort the cooperative recommendations directly above.\n",
    "\n",
    "2. Explore the effect of different distance metrics on the recommendations.  The 'euclidean', 'manhattan', and 'cosine' metrics are some of the more common ones.  Check the `NearestNeighbors` documentation for additional options.\n",
    "\n",
    "3. Add in the year as a feature of movies.  Should it be a continuous or categorical variable?  While it's likely that a user who likes movies from 1970 will also like movies from 1971 (suggesting a continuous feature), a user liking movies from 1970 and 1990 is no guarantee that they like movies from 1980 (suggesting a categorical feature).  Can you find an encoding that's in between these two options?  How does the scale of the year, if treated as a continuous feature, affect the KNN calculation?  You may want to use a `StandardScaler`, from `sklearn.preprocessing`, to reduce this effect.\n",
    "\n",
    "4. In weighting reviews, we consider not reviewing a movie to be less of a recommendation that scoring it a one.  Shift the rating scale to change this, and see how the affects the resultant recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
